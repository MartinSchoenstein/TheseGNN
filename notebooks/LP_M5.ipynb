{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3196b7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.utils import from_networkx\n",
    "import torch_geometric.transforms as T\n",
    "import random\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.nn import Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "457c8df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_gml(\"/home/schoenstein/these/gnn_postbiblio/graph/graph_light.gml\")\n",
    "data = from_networkx(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "836aa2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 664450], num_nodes=116979, edge_label=[664450], edge_label_index=[2, 664450])\n",
      "Data(edge_index=[2, 664450], num_nodes=116979, edge_label=[83056], edge_label_index=[2, 83056])\n",
      "Data(edge_index=[2, 747506], num_nodes=116979, edge_label=[83056], edge_label_index=[2, 83056])\n"
     ]
    }
   ],
   "source": [
    "train = \"random\"\n",
    "\n",
    "if train == \"random\":\n",
    "    transform = T.RandomLinkSplit(\n",
    "        num_val = 0.1,  \n",
    "        num_test = 0.1,  \n",
    "        disjoint_train_ratio = 0,  \n",
    "        neg_sampling_ratio = 1,\n",
    "        is_undirected = True\n",
    "    )\n",
    "    train_data, val_data, test_data = transform(data)\n",
    "    print(train_data)\n",
    "    print(val_data)\n",
    "    print(test_data)\n",
    "\n",
    "elif train == \"double split\":\n",
    "    cc = list(nx.connected_components(G))\n",
    "    neg_inside = 0\n",
    "    train_list = []\n",
    "    val_list = []\n",
    "    test_list = []\n",
    "    for c in cc:\n",
    "        G2 = G.subgraph(c).copy()\n",
    "        data2 = from_networkx(G2)\n",
    "        transform = T.RandomLinkSplit(\n",
    "                num_val = 0.1,  \n",
    "                num_test = 0.1,  \n",
    "                disjoint_train_ratio = 0,  \n",
    "                neg_sampling_ratio = 1.5,\n",
    "                is_undirected = True\n",
    "            )\n",
    "        train_data2, val_data2, test_data2 = transform(data2)\n",
    "        neg_inside = neg_inside + len(train_data2.edge_label)\n",
    "        train_list.append(train_data2)\n",
    "        val_list.append(val_data2)\n",
    "        test_list.append(test_data2)\n",
    "    ratio_neg_inside = neg_inside/(len(list(G.edges()))*2)\n",
    "    print(ratio_neg_inside)\n",
    "    transform = T.RandomLinkSplit(\n",
    "            num_val = 0.1,  \n",
    "            num_test = 0.1,  \n",
    "            disjoint_train_ratio = 0,  \n",
    "            neg_sampling_ratio = 1 - ratio_neg_inside,\n",
    "            is_undirected = True\n",
    "        )\n",
    "    train_data3, val_data3, test_data3 = transform(data)\n",
    "\n",
    "    #pos_train = torch.cat([d.edge_label_index[:, d.edge_label==1] \n",
    "                             #for d in train_list], dim=1)\n",
    "    pos_train = train_data3.edge_label_index[:, train_data3.edge_label == 1]\n",
    "    #pos_val = torch.cat([d.edge_label_index[:, d.edge_label==1] \n",
    "                           #for d in val_list], dim=1)\n",
    "    pos_val = val_data3.edge_label_index[:, val_data3.edge_label == 1]\n",
    "    #pos_test = torch.cat([d.edge_label_index[:, d.edge_label==1] \n",
    "                            #for d in test_list], dim=1)\n",
    "    pos_test = test_data3.edge_label_index[:, test_data3.edge_label == 1]\n",
    "    neg_train1 = torch.cat([d.edge_label_index[:, d.edge_label==0] \n",
    "                             for d in train_list], dim=1)\n",
    "    neg_val1 = torch.cat([d.edge_label_index[:, d.edge_label==0] \n",
    "                           for d in val_list], dim=1)\n",
    "    neg_test1 = torch.cat([d.edge_label_index[:, d.edge_label==0] \n",
    "                            for d in test_list], dim=1)\n",
    "    neg_train2 = train_data3.edge_label_index[:, train_data3.edge_label==0]             \n",
    "    neg_val2 = val_data3.edge_label_index[:, val_data3.edge_label==0] \n",
    "    neg_test2 = test_data3.edge_label_index[:, test_data3.edge_label==0]\n",
    "    neg_train = torch.cat([neg_train1, neg_train2], dim=1)\n",
    "    neg_val = torch.cat([neg_val1, neg_val2], dim=1)\n",
    "    neg_test = torch.cat([neg_test1, neg_test2], dim=1)\n",
    "    train_data = Data(\n",
    "        edge_index=train_data3.edge_index,\n",
    "        num_nodes=data.num_nodes,\n",
    "        edge_label_index=torch.cat([pos_train, neg_train], dim=1),\n",
    "        edge_label=torch.cat([\n",
    "            torch.ones(pos_train.size(1), dtype=torch.long),\n",
    "            torch.zeros(neg_train.size(1), dtype=torch.long)\n",
    "        ])\n",
    "    )\n",
    "    val_data = Data(\n",
    "        edge_index=val_data3.edge_index,\n",
    "        num_nodes=data.num_nodes,\n",
    "        edge_label_index=torch.cat([pos_val, neg_val], dim=1),\n",
    "        edge_label=torch.cat([\n",
    "            torch.ones(pos_val.size(1), dtype=torch.long),\n",
    "            torch.zeros(neg_val.size(1), dtype=torch.long)\n",
    "        ])\n",
    "    )\n",
    "    test_data = Data(\n",
    "        edge_index=test_data3.edge_index,\n",
    "        num_nodes=data.num_nodes,\n",
    "        edge_label_index=torch.cat([pos_test, neg_test], dim=1),\n",
    "        edge_label=torch.cat([\n",
    "            torch.ones(pos_test.size(1), dtype=torch.long),\n",
    "            torch.zeros(neg_test.size(1), dtype=torch.long)\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    print(train_data)\n",
    "    print(val_data)\n",
    "    print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bd890ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.x = torch.ones((train_data.num_nodes, 1))\n",
    "val_data.x = train_data.x.clone()\n",
    "test_data.x = train_data.x.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd4f3ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_n2v = Node2Vec(\n",
    "    edge_index = train_data.edge_index,\n",
    "    embedding_dim = 64,\n",
    "    walk_length = 20,\n",
    "    context_size = 10,\n",
    "    walks_per_node = 10,\n",
    "    p = 1.0,\n",
    "    q = 1.0,\n",
    "    num_negative_samples = 1,\n",
    "    sparse=False  #True : seuls les noeuds vus sont mis à jour, False : toute la matrice reçoit un gradient donc moins opti mais nécessaire pour entrainer les 2 en même temps\n",
    ")\n",
    "n2v_loader = model_n2v.loader(batch_size = 128, shuffle =  True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fde53c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_channels, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_label_index):\n",
    "        edge_emb_src = x[edge_label_index[0]]\n",
    "        edge_emb_dst = x[edge_label_index[1]]\n",
    "        edge_emb = torch.cat([edge_emb_src, edge_emb_dst], dim=-1)\n",
    "        return self.mlp(edge_emb).view(-1)\n",
    "\n",
    "predictor = Predictor(hidden_channels=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6359259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer_n2v = torch.optim.SparseAdam(model_n2v.parameters(), lr=0.01)\n",
    "optimizer = torch.optim.Adam(list(model_n2v.parameters()) + list(predictor.parameters()),lr=0.01)\n",
    "#optimizer_mlp = torch.optim.Adam(predictor.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c41445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch():\n",
    "    model_n2v.train()\n",
    "    predictor.train()\n",
    "    #optimizer_n2v.zero_grad()\n",
    "    #optimizer_mlp.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "    loss_n2v = 0\n",
    "    count = 0\n",
    "    for pos, neg in n2v_loader:\n",
    "        loss = model_n2v.loss(pos, neg)\n",
    "        loss_n2v = loss_n2v + loss.item()\n",
    "        count = count + 1\n",
    "    loss_n2v = loss_n2v / count\n",
    "    z = model_n2v()   \n",
    "    pred = predictor(z, train_data.edge_label_index)\n",
    "    loss_lp = F.binary_cross_entropy_with_logits(pred, train_data.edge_label.float())\n",
    "    loss_total = loss_n2v + loss_lp\n",
    "    loss_total.backward()\n",
    "    #optimizer_n2v.step()\n",
    "    #optimizer_mlp.step()\n",
    "    optimizer.step()\n",
    "    return loss_n2v, loss_lp\n",
    "\n",
    "def evaluate():\n",
    "    model_n2v.eval()\n",
    "    predictor.eval()\n",
    "    z = model_n2v()\n",
    "    y_pred = predictor(z, val_data.edge_label_index)\n",
    "    loss = F.binary_cross_entropy_with_logits(y_pred, val_data.edge_label.float()).item()\n",
    "    auc = roc_auc_score(val_data.edge_label.detach().numpy(),y_pred.detach().numpy())\n",
    "    ap = average_precision_score(val_data.edge_label.detach().numpy(),y_pred.detach().numpy())\n",
    "    return loss, auc, ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69420726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001, N2V Loss: 6.3733, LP Loss : 0.6971, Val Loss : 0.6954, Val AUC: 0.5038, Val AP: 0.5025\n",
      "Epoch 002, N2V Loss: 6.3732, LP Loss : 0.6944, Val Loss : 0.6946, Val AUC: 0.5080, Val AP: 0.5067\n",
      "Epoch 003, N2V Loss: 6.3721, LP Loss : 0.6933, Val Loss : 0.6941, Val AUC: 0.5123, Val AP: 0.5109\n",
      "Epoch 004, N2V Loss: 6.3723, LP Loss : 0.6924, Val Loss : 0.6933, Val AUC: 0.5162, Val AP: 0.5154\n",
      "Epoch 005, N2V Loss: 6.3714, LP Loss : 0.6912, Val Loss : 0.6930, Val AUC: 0.5196, Val AP: 0.5196\n",
      "Epoch 006, N2V Loss: 6.3733, LP Loss : 0.6905, Val Loss : 0.6926, Val AUC: 0.5228, Val AP: 0.5234\n",
      "Epoch 007, N2V Loss: 6.3704, LP Loss : 0.6896, Val Loss : 0.6922, Val AUC: 0.5255, Val AP: 0.5266\n",
      "Epoch 008, N2V Loss: 6.3716, LP Loss : 0.6888, Val Loss : 0.6921, Val AUC: 0.5277, Val AP: 0.5291\n",
      "Epoch 009, N2V Loss: 6.3712, LP Loss : 0.6881, Val Loss : 0.6919, Val AUC: 0.5298, Val AP: 0.5314\n",
      "Epoch 010, N2V Loss: 6.3733, LP Loss : 0.6873, Val Loss : 0.6916, Val AUC: 0.5323, Val AP: 0.5340\n",
      "Epoch 011, N2V Loss: 6.3719, LP Loss : 0.6863, Val Loss : 0.6914, Val AUC: 0.5350, Val AP: 0.5369\n",
      "Epoch 012, N2V Loss: 6.3715, LP Loss : 0.6852, Val Loss : 0.6912, Val AUC: 0.5379, Val AP: 0.5401\n",
      "Epoch 013, N2V Loss: 6.3734, LP Loss : 0.6840, Val Loss : 0.6908, Val AUC: 0.5407, Val AP: 0.5437\n",
      "Epoch 014, N2V Loss: 6.3736, LP Loss : 0.6825, Val Loss : 0.6904, Val AUC: 0.5436, Val AP: 0.5474\n",
      "Epoch 015, N2V Loss: 6.3717, LP Loss : 0.6808, Val Loss : 0.6900, Val AUC: 0.5466, Val AP: 0.5514\n",
      "Epoch 016, N2V Loss: 6.3719, LP Loss : 0.6789, Val Loss : 0.6895, Val AUC: 0.5498, Val AP: 0.5555\n",
      "Epoch 017, N2V Loss: 6.3724, LP Loss : 0.6767, Val Loss : 0.6890, Val AUC: 0.5531, Val AP: 0.5595\n",
      "Epoch 018, N2V Loss: 6.3704, LP Loss : 0.6742, Val Loss : 0.6886, Val AUC: 0.5564, Val AP: 0.5633\n",
      "Epoch 019, N2V Loss: 6.3715, LP Loss : 0.6714, Val Loss : 0.6882, Val AUC: 0.5597, Val AP: 0.5670\n",
      "Epoch 020, N2V Loss: 6.3720, LP Loss : 0.6683, Val Loss : 0.6880, Val AUC: 0.5631, Val AP: 0.5708\n",
      "Epoch 021, N2V Loss: 6.3726, LP Loss : 0.6648, Val Loss : 0.6877, Val AUC: 0.5666, Val AP: 0.5750\n",
      "Epoch 022, N2V Loss: 6.3716, LP Loss : 0.6610, Val Loss : 0.6876, Val AUC: 0.5701, Val AP: 0.5794\n",
      "Epoch 023, N2V Loss: 6.3732, LP Loss : 0.6567, Val Loss : 0.6877, Val AUC: 0.5736, Val AP: 0.5838\n",
      "Epoch 024, N2V Loss: 6.3715, LP Loss : 0.6521, Val Loss : 0.6881, Val AUC: 0.5773, Val AP: 0.5880\n",
      "Epoch 025, N2V Loss: 6.3734, LP Loss : 0.6470, Val Loss : 0.6884, Val AUC: 0.5809, Val AP: 0.5921\n",
      "Epoch 026, N2V Loss: 6.3756, LP Loss : 0.6415, Val Loss : 0.6888, Val AUC: 0.5845, Val AP: 0.5963\n",
      "Epoch 027, N2V Loss: 6.3747, LP Loss : 0.6355, Val Loss : 0.6895, Val AUC: 0.5883, Val AP: 0.6007\n",
      "Epoch 028, N2V Loss: 6.3754, LP Loss : 0.6291, Val Loss : 0.6908, Val AUC: 0.5920, Val AP: 0.6049\n",
      "Epoch 029, N2V Loss: 6.3736, LP Loss : 0.6222, Val Loss : 0.6922, Val AUC: 0.5956, Val AP: 0.6089\n",
      "Epoch 030, N2V Loss: 6.3758, LP Loss : 0.6147, Val Loss : 0.6938, Val AUC: 0.5994, Val AP: 0.6130\n",
      "Epoch 031, N2V Loss: 6.3736, LP Loss : 0.6068, Val Loss : 0.6955, Val AUC: 0.6034, Val AP: 0.6172\n",
      "Epoch 032, N2V Loss: 6.3750, LP Loss : 0.5984, Val Loss : 0.6977, Val AUC: 0.6071, Val AP: 0.6212\n",
      "Epoch 033, N2V Loss: 6.3783, LP Loss : 0.5894, Val Loss : 0.7010, Val AUC: 0.6108, Val AP: 0.6251\n",
      "Epoch 034, N2V Loss: 6.3776, LP Loss : 0.5799, Val Loss : 0.7033, Val AUC: 0.6148, Val AP: 0.6289\n",
      "Epoch 035, N2V Loss: 6.3793, LP Loss : 0.5699, Val Loss : 0.7068, Val AUC: 0.6184, Val AP: 0.6326\n",
      "Epoch 036, N2V Loss: 6.3805, LP Loss : 0.5594, Val Loss : 0.7100, Val AUC: 0.6221, Val AP: 0.6363\n",
      "Epoch 037, N2V Loss: 6.3788, LP Loss : 0.5483, Val Loss : 0.7151, Val AUC: 0.6260, Val AP: 0.6400\n",
      "Epoch 038, N2V Loss: 6.3822, LP Loss : 0.5369, Val Loss : 0.7185, Val AUC: 0.6293, Val AP: 0.6431\n",
      "Epoch 039, N2V Loss: 6.3828, LP Loss : 0.5250, Val Loss : 0.7250, Val AUC: 0.6332, Val AP: 0.6466\n",
      "Epoch 040, N2V Loss: 6.3825, LP Loss : 0.5130, Val Loss : 0.7263, Val AUC: 0.6360, Val AP: 0.6494\n",
      "Epoch 041, N2V Loss: 6.3836, LP Loss : 0.5004, Val Loss : 0.7319, Val AUC: 0.6399, Val AP: 0.6526\n",
      "Epoch 042, N2V Loss: 6.3860, LP Loss : 0.4869, Val Loss : 0.7354, Val AUC: 0.6431, Val AP: 0.6555\n",
      "Epoch 043, N2V Loss: 6.3882, LP Loss : 0.4735, Val Loss : 0.7385, Val AUC: 0.6457, Val AP: 0.6579\n",
      "Epoch 044, N2V Loss: 6.3876, LP Loss : 0.4603, Val Loss : 0.7452, Val AUC: 0.6492, Val AP: 0.6606\n",
      "Epoch 045, N2V Loss: 6.3891, LP Loss : 0.4462, Val Loss : 0.7506, Val AUC: 0.6520, Val AP: 0.6628\n",
      "Epoch 046, N2V Loss: 6.3927, LP Loss : 0.4321, Val Loss : 0.7558, Val AUC: 0.6545, Val AP: 0.6648\n",
      "Epoch 047, N2V Loss: 6.3941, LP Loss : 0.4180, Val Loss : 0.7644, Val AUC: 0.6573, Val AP: 0.6671\n",
      "Epoch 048, N2V Loss: 6.3948, LP Loss : 0.4034, Val Loss : 0.7722, Val AUC: 0.6597, Val AP: 0.6688\n",
      "Epoch 049, N2V Loss: 6.3985, LP Loss : 0.3888, Val Loss : 0.7800, Val AUC: 0.6618, Val AP: 0.6702\n"
     ]
    }
   ],
   "source": [
    "best_val_auc = 0\n",
    "limit = 6\n",
    "count = 0\n",
    "for epoch in range(1, 50):\n",
    "    loss_n2v, loss_lp = train_epoch()\n",
    "    val_loss, val_auc, val_ap = evaluate()\n",
    "    print(f\"Epoch {epoch:03d}, N2V Loss: {loss_n2v:.4f}, LP Loss : {loss_lp:.4f}, Val Loss : {val_loss:.4f}, Val AUC: {val_auc:.4f}, Val AP: {val_ap:.4f}\")\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        count = 0\n",
    "    else:\n",
    "        count += 1\n",
    "        if count >= limit:\n",
    "            print(\"Early stop\")\n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
